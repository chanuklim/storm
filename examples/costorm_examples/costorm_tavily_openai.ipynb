{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Co-STORM quick test (OpenAI + Tavily)\n",
        "\n",
        "This notebook exercises the Co-STORM pipeline using **OpenAI** for the LLMs and **Tavily** for retrieval.\n",
        "\n",
        "**Prerequisites**\n",
        "- `OPENAI_API_KEY` set in the environment\n",
        "- `TAVILY_API_KEY` set in the environment\n",
        "- `OPENAI_API_TYPE` set to `openai` (optional; defaults to `openai` below)\n",
        "\n",
        "You can also place keys in `secrets.toml` and call `load_api_key()` if preferred.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "from knowledge_storm.collaborative_storm.engine import (\n",
        "    CollaborativeStormLMConfigs,\n",
        "    RunnerArgument,\n",
        "    CoStormRunner,\n",
        ")\n",
        "from knowledge_storm.lm import OpenAIModel\n",
        "from knowledge_storm.logging_wrapper import LoggingWrapper\n",
        "from knowledge_storm.rm import TavilySearchRM\n",
        "from knowledge_storm.utils import load_api_key\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: load from secrets.toml if present\n",
        "# load_api_key(toml_file_path=\"secrets.toml\")\n",
        "\n",
        "os.environ.setdefault(\"OPENAI_API_TYPE\", \"openai\")\n",
        "\n",
        "if os.getenv(\"OPENAI_API_TYPE\") != \"openai\":\n",
        "    raise ValueError(\"This notebook expects OPENAI_API_TYPE=openai.\")\n",
        "\n",
        "if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "    raise ValueError(\"Missing OPENAI_API_KEY in the environment.\")\n",
        "if not os.getenv(\"TAVILY_API_KEY\"):\n",
        "    raise ValueError(\"Missing TAVILY_API_KEY in the environment.\")\n",
        "\n",
        "topic = \"Recent advances in battery recycling\"\n",
        "user_utterance = \"Focus on policy changes in the last two years.\"\n",
        "output_dir = \"./results/co-storm-notebook\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure Co-STORM LMs (all using OpenAI in this setup)\n",
        "lm_config = CollaborativeStormLMConfigs()\n",
        "openai_kwargs = {\n",
        "    \"api_key\": os.getenv(\"OPENAI_API_KEY\"),\n",
        "    \"api_provider\": \"openai\",\n",
        "    \"temperature\": 1.0,\n",
        "    \"top_p\": 0.9,\n",
        "    \"api_base\": None,\n",
        "}\n",
        "\n",
        "gpt_4o_model_name = \"gpt-4o\"\n",
        "\n",
        "question_answering_lm = OpenAIModel(model=gpt_4o_model_name, max_tokens=1000, **openai_kwargs)\n",
        "discourse_manage_lm = OpenAIModel(model=gpt_4o_model_name, max_tokens=500, **openai_kwargs)\n",
        "utterance_polishing_lm = OpenAIModel(model=gpt_4o_model_name, max_tokens=2000, **openai_kwargs)\n",
        "warmstart_outline_gen_lm = OpenAIModel(model=gpt_4o_model_name, max_tokens=500, **openai_kwargs)\n",
        "question_asking_lm = OpenAIModel(model=gpt_4o_model_name, max_tokens=300, **openai_kwargs)\n",
        "knowledge_base_lm = OpenAIModel(model=gpt_4o_model_name, max_tokens=1000, **openai_kwargs)\n",
        "\n",
        "lm_config.set_question_answering_lm(question_answering_lm)\n",
        "lm_config.set_discourse_manage_lm(discourse_manage_lm)\n",
        "lm_config.set_utterance_polishing_lm(utterance_polishing_lm)\n",
        "lm_config.set_warmstart_outline_gen_lm(warmstart_outline_gen_lm)\n",
        "lm_config.set_question_asking_lm(question_asking_lm)\n",
        "lm_config.set_knowledge_base_lm(knowledge_base_lm)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure runner + retriever\n",
        "runner_argument = RunnerArgument(\n",
        "    topic=topic,\n",
        "    retrieve_top_k=5,\n",
        "    max_search_queries=2,\n",
        "    total_conv_turn=10,\n",
        "    max_search_thread=3,\n",
        "    max_search_queries_per_turn=2,\n",
        "    warmstart_max_num_experts=2,\n",
        "    warmstart_max_turn_per_experts=2,\n",
        "    warmstart_max_thread=2,\n",
        "    max_thread_num=5,\n",
        "    max_num_round_table_experts=2,\n",
        "    moderator_override_N_consecutive_answering_turn=2,\n",
        "    node_expansion_trigger_count=10,\n",
        ")\n",
        "\n",
        "rm = TavilySearchRM(\n",
        "    tavily_search_api_key=os.getenv(\"TAVILY_API_KEY\"),\n",
        "    k=runner_argument.retrieve_top_k,\n",
        "    include_raw_content=True,\n",
        ")\n",
        "\n",
        "logging_wrapper = LoggingWrapper(lm_config)\n",
        "costorm_runner = CoStormRunner(\n",
        "    lm_config=lm_config,\n",
        "    runner_argument=runner_argument,\n",
        "    logging_wrapper=logging_wrapper,\n",
        "    rm=rm,\n",
        ")\n",
        "\n",
        "# Warm start the system to build shared conceptual space\n",
        "costorm_runner.warm_start()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Observe one turn from the system\n",
        "conv_turn = costorm_runner.step()\n",
        "print(f\"**{conv_turn.role}**: {conv_turn.utterance}\\n\")\n",
        "\n",
        "# Inject a user utterance\n",
        "costorm_runner.step(user_utterance=user_utterance)\n",
        "\n",
        "# Observe another turn\n",
        "conv_turn = costorm_runner.step()\n",
        "print(f\"**{conv_turn.role}**: {conv_turn.utterance}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate a report and save artifacts\n",
        "costorm_runner.knowledge_base.reorganize()\n",
        "article = costorm_runner.generate_report()\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "with open(os.path.join(output_dir, \"report.md\"), \"w\") as f:\n",
        "    f.write(article)\n",
        "\n",
        "instance_copy = costorm_runner.to_dict()\n",
        "with open(os.path.join(output_dir, \"instance_dump.json\"), \"w\") as f:\n",
        "    json.dump(instance_copy, f, indent=2)\n",
        "\n",
        "log_dump = costorm_runner.dump_logging_and_reset()\n",
        "with open(os.path.join(output_dir, \"log.json\"), \"w\") as f:\n",
        "    json.dump(log_dump, f, indent=2)\n",
        "\n",
        "print(f\"Artifacts saved to: {output_dir}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
