{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "461c3de7",
   "metadata": {},
   "source": [
    "# Co-STORM (English-internal pipeline with dual reports)\n",
    "\n",
    "This notebook mirrors `run_costorm_gpt.py` with the English-internal flow:\n",
    "- User input language is detected; Korean inputs are translated to English for processing.\n",
    "- All internal prompts/responses remain in English.\n",
    "- Retriever queries are English.\n",
    "- Model utterances shown to the user are translated back to Korean if the user asked in Korean.\n",
    "- Final reports: `report_eng.md` (English), and if the user language was Korean, a translated `report_kr.md`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8586c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import traceback\n",
    "from argparse import ArgumentParser\n",
    "from typing import Optional\n",
    "from pathlib import Path\n",
    "import importlib\n",
    "\n",
    "# Force repo root for imports so we use the local knowledge_storm implementation.\n",
    "REPO_ROOT = Path('/data/coscientist/storm')\n",
    "if str(REPO_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(REPO_ROOT))\n",
    "os.environ.setdefault('PYTHONPATH', str(REPO_ROOT))\n",
    "\n",
    "# Purge any previously imported knowledge_storm modules to force re-import from REPO_ROOT\n",
    "for mod in list(sys.modules.keys()):\n",
    "    if mod.startswith('knowledge_storm'):\n",
    "        sys.modules.pop(mod)\n",
    "\n",
    "from knowledge_storm.collaborative_storm.engine import (\n",
    "    CollaborativeStormLMConfigs,\n",
    "    RunnerArgument,\n",
    "    CoStormRunner,\n",
    ")\n",
    "from knowledge_storm.collaborative_storm.modules.callback import (\n",
    "    LocalConsolePrintCallBackHandler,\n",
    ")\n",
    "from knowledge_storm.collaborative_storm.modules.collaborative_storm_utils import (\n",
    "    detect_language,\n",
    "    translate_text,\n",
    ")\n",
    "from knowledge_storm.lm import LitellmModel, OpenAIModel, AzureOpenAIModel\n",
    "from knowledge_storm.logging_wrapper import LoggingWrapper\n",
    "from knowledge_storm.rm import (\n",
    "    YouRM,\n",
    "    BingSearch,\n",
    "    BraveRM,\n",
    "    SerperRM,\n",
    "    DuckDuckGoSearchRM,\n",
    "    TavilySearchRM,\n",
    "    SearXNG,\n",
    ")\n",
    "from knowledge_storm.encoder import Encoder\n",
    "from knowledge_storm.utils import load_api_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f530e292",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Co-STORM pipeline with support for local Ollama (default), OpenAI, or Azure models and multiple search engines.\n",
    "\n",
    "Key environment variables when needed:\n",
    "    - OPENAI_API_KEY: OpenAI API key (if --llm-provider openai)\n",
    "    - AZURE_API_KEY / AZURE_API_BASE / AZURE_API_VERSION: Azure API config (if --llm-provider azure)\n",
    "    - BING_SEARCH_API_KEY / SERPER_API_KEY / BRAVE_API_KEY / TAVILY_API_KEY / etc.: Retriever keys\n",
    "    - OLLAMA_MODELS: Optional, directory for Ollama models (defaults to --ollama-model-dir)\n",
    "    - HF_HOME: Optional, cache dir for local embedding models (defaults to --embedding-cache-dir)\n",
    "\n",
    "Output will be structured as below\n",
    "args.output_dir/\n",
    "    log.json           # Log of information-seeking conversation\n",
    "    report.md          # Final article generated\n",
    "    instance_dump.json # Serialized run state\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import traceback\n",
    "from argparse import ArgumentParser\n",
    "from typing import Optional\n",
    "from pathlib import Path\n",
    "\n",
    "# Ensure repository root is on sys.path so local knowledge_storm is used even if an older package is installed.\n",
    "try:\n",
    "    REPO_ROOT = Path(__file__).resolve().parents[2]\n",
    "except NameError:\n",
    "    REPO_ROOT = Path.cwd()\n",
    "if str(REPO_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(REPO_ROOT))\n",
    "from knowledge_storm.collaborative_storm.engine import (\n",
    "    CollaborativeStormLMConfigs,\n",
    "    RunnerArgument,\n",
    "    CoStormRunner,\n",
    ")\n",
    "from knowledge_storm.collaborative_storm.modules.callback import (\n",
    "    LocalConsolePrintCallBackHandler,\n",
    ")\n",
    "from knowledge_storm.collaborative_storm.modules.collaborative_storm_utils import (\n",
    "    detect_language,\n",
    "    translate_text,\n",
    ")\n",
    "from knowledge_storm.lm import LitellmModel, OpenAIModel, AzureOpenAIModel\n",
    "from knowledge_storm.logging_wrapper import LoggingWrapper\n",
    "from knowledge_storm.rm import (\n",
    "    YouRM,\n",
    "    BingSearch,\n",
    "    BraveRM,\n",
    "    SerperRM,\n",
    "    DuckDuckGoSearchRM,\n",
    "    TavilySearchRM,\n",
    "    SearXNG,\n",
    ")\n",
    "from knowledge_storm.encoder import Encoder\n",
    "from knowledge_storm.utils import load_api_key\n",
    "\n",
    "\n",
    "def build_base_url(url: str, port: Optional[int] = None) -> str:\n",
    "    \"\"\"Normalize base URL and optionally append port.\"\"\"\n",
    "    if not url.startswith(\"http://\") and not url.startswith(\"https://\"):\n",
    "        url = f\"http://{url}\"\n",
    "    url = url.rstrip(\"/\")\n",
    "    if port and f\":{port}\" not in url.split(\"//\", 1)[-1]:\n",
    "        url = f\"{url}:{port}\"\n",
    "    return url\n",
    "\n",
    "\n",
    "def chunked_translate_report(\n",
    "    translator_lm,\n",
    "    text: str,\n",
    "    target_lang: str = \"ko\",\n",
    "    source_lang_hint: str = \"en\",\n",
    "    max_chunk_chars: int = 2000,\n",
    "    log_path: Optional[str] = None,\n",
    "):\n",
    "    \"\"\"Translate long reports in chunks to reduce context-related failures.\"\"\"\n",
    "    if not text:\n",
    "        return text\n",
    "    paragraphs = text.split(\"\\n\\n\")\n",
    "    # Pair headings with the following paragraph to preserve structure.\n",
    "    paired_paragraphs = []\n",
    "    skip_next = False\n",
    "    for idx, para in enumerate(paragraphs):\n",
    "        if skip_next:\n",
    "            skip_next = False\n",
    "            continue\n",
    "        if para.strip().startswith(\"#\") and idx + 1 < len(paragraphs):\n",
    "            paired_paragraphs.append(f\"{para}\\n\\n{paragraphs[idx + 1]}\")\n",
    "            skip_next = True\n",
    "        else:\n",
    "            paired_paragraphs.append(para)\n",
    "\n",
    "    chunks = []\n",
    "    buffer = \"\"\n",
    "    for para in paired_paragraphs:\n",
    "        # If a single paragraph is too large, break it up by character count.\n",
    "        if len(para) > max_chunk_chars:\n",
    "            if buffer:\n",
    "                chunks.append(buffer)\n",
    "                buffer = \"\"\n",
    "            for i in range(0, len(para), max_chunk_chars):\n",
    "                chunks.append(para[i : i + max_chunk_chars])\n",
    "            continue\n",
    "        if len(buffer) + len(para) + 2 <= max_chunk_chars:\n",
    "            buffer = para if not buffer else f\"{buffer}\\n\\n{para}\"\n",
    "        else:\n",
    "            chunks.append(buffer)\n",
    "            buffer = para\n",
    "    if buffer:\n",
    "        chunks.append(buffer)\n",
    "\n",
    "    translated_chunks = []\n",
    "    log_entries = []\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        if not chunk.strip():\n",
    "            log_entries.append({\"chunk_index\": idx, \"status\": \"skipped_empty\"})\n",
    "            continue\n",
    "        prompt = (\n",
    "            f\"Translate the following Markdown to {target_lang}. \"\n",
    "            \"Keep the Markdown structure exactly and do not add any prefixes, notes, or explanations. \"\n",
    "            \"Only return the translated Markdown content.\\n\"\n",
    "            f\"Source language hint: {source_lang_hint}.\\n\\n\"\n",
    "            f\"Text:\\n{chunk}\\n\\nTranslation:\"\n",
    "        )\n",
    "        translated = translator_lm(prompt)[0].strip()\n",
    "        status = \"translated\"\n",
    "        if not translated:\n",
    "            translated = chunk\n",
    "            status = \"fallback_original\"\n",
    "        # Strip common artifacts the model might add.\n",
    "        translated = translated.replace(\"Translated (ko):\", \"\").strip()\n",
    "        translated_chunks.append(translated)\n",
    "        log_entries.append(\n",
    "            {\n",
    "                \"chunk_index\": idx,\n",
    "                \"input_chars\": len(chunk),\n",
    "                \"output_chars\": len(translated),\n",
    "                \"status\": status,\n",
    "            }\n",
    "        )\n",
    "    if log_path:\n",
    "        try:\n",
    "            with open(log_path, \"w\") as lf:\n",
    "                json.dump(log_entries, lf, ensure_ascii=False, indent=2)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return \"\\n\\n\".join(translated_chunks)\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    load_api_key(toml_file_path=args.secrets_file)\n",
    "    lm_config: CollaborativeStormLMConfigs = CollaborativeStormLMConfigs()\n",
    "    if args.llm_provider == \"ollama\" and args.ollama_model_dir:\n",
    "        os.environ.setdefault(\"OLLAMA_MODELS\", args.ollama_model_dir)\n",
    "\n",
    "    if args.encoder_type == \"hf_local\" and args.embedding_cache_dir:\n",
    "        os.environ.setdefault(\"HF_HOME\", args.embedding_cache_dir)\n",
    "\n",
    "    embedding_base_url = (\n",
    "        build_base_url(args.embedding_base_url, args.embedding_port)\n",
    "        if args.encoder_type == \"ollama\"\n",
    "        else None\n",
    "    )\n",
    "    encoder_device = None if args.embedding_device == \"auto\" else args.embedding_device\n",
    "    encoder = Encoder(\n",
    "        encoder_type=args.encoder_type,\n",
    "        api_base=embedding_base_url,\n",
    "        model=args.embedding_model,\n",
    "        device=encoder_device,\n",
    "        cache_dir=args.embedding_cache_dir if args.encoder_type == \"hf_local\" else None,\n",
    "    )\n",
    "\n",
    "    llm_provider = args.llm_provider.lower()\n",
    "    if llm_provider == \"ollama\":\n",
    "        llm_base_url = build_base_url(args.llm_url, args.llm_port)\n",
    "        model_name = args.llm_model\n",
    "        if not model_name.startswith(\"ollama/\"):\n",
    "            model_name = f\"ollama/{model_name}\"\n",
    "        ollama_kwargs = {\n",
    "            \"base_url\": llm_base_url,\n",
    "            \"temperature\": args.llm_temperature,\n",
    "            \"top_p\": args.llm_top_p,\n",
    "            \"model_type\": \"chat\",\n",
    "        }\n",
    "\n",
    "        def build_lm(max_tokens: int):\n",
    "            return LitellmModel(\n",
    "                model=model_name,\n",
    "                max_tokens=max_tokens,\n",
    "                **ollama_kwargs,\n",
    "            )\n",
    "\n",
    "    elif llm_provider == \"openai\":\n",
    "        openai_kwargs = {\n",
    "            \"api_key\": os.getenv(\"OPENAI_API_KEY\"),\n",
    "            \"api_provider\": \"openai\",\n",
    "            \"temperature\": args.llm_temperature,\n",
    "            \"top_p\": args.llm_top_p,\n",
    "            \"api_base\": None,\n",
    "        }\n",
    "        ModelClass = OpenAIModel\n",
    "        gpt_4o_model_name = \"gpt-4o\"\n",
    "\n",
    "        def build_lm(max_tokens: int):\n",
    "            return ModelClass(\n",
    "                model=gpt_4o_model_name, max_tokens=max_tokens, **openai_kwargs\n",
    "            )\n",
    "\n",
    "    elif llm_provider == \"azure\":\n",
    "        openai_kwargs = {\n",
    "            \"api_key\": os.getenv(\"AZURE_API_KEY\"),\n",
    "            \"temperature\": args.llm_temperature,\n",
    "            \"top_p\": args.llm_top_p,\n",
    "            \"api_base\": os.getenv(\"AZURE_API_BASE\"),\n",
    "            \"api_version\": os.getenv(\"AZURE_API_VERSION\"),\n",
    "        }\n",
    "        ModelClass = AzureOpenAIModel\n",
    "        gpt_4o_model_name = \"gpt-4o\"\n",
    "\n",
    "        def build_lm(max_tokens: int):\n",
    "            return ModelClass(\n",
    "                model=gpt_4o_model_name, max_tokens=max_tokens, **openai_kwargs\n",
    "            )\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f'Invalid llm provider: {args.llm_provider}. Choose either \"ollama\", \"openai\", or \"azure\".'\n",
    "        )\n",
    "\n",
    "    # STORM is a LM system so different components can be powered by different models.\n",
    "    question_answering_lm = build_lm(1000)\n",
    "    discourse_manage_lm = build_lm(500)\n",
    "    utterance_polishing_lm = build_lm(2000)\n",
    "    warmstart_outline_gen_lm = build_lm(500)\n",
    "    question_asking_lm = build_lm(300)\n",
    "    knowledge_base_lm = build_lm(1000)\n",
    "\n",
    "    # Use a separate translator model so user-facing translations don't pollute LM history\n",
    "    translator_lm = build_lm(500)\n",
    "\n",
    "    lm_config.set_question_answering_lm(question_answering_lm)\n",
    "    lm_config.set_discourse_manage_lm(discourse_manage_lm)\n",
    "    lm_config.set_utterance_polishing_lm(utterance_polishing_lm)\n",
    "    lm_config.set_warmstart_outline_gen_lm(warmstart_outline_gen_lm)\n",
    "    lm_config.set_question_asking_lm(question_asking_lm)\n",
    "    lm_config.set_knowledge_base_lm(knowledge_base_lm)\n",
    "\n",
    "    topic_raw = input(\"Topic: \")\n",
    "    user_lang = detect_language(topic_raw)\n",
    "    topic = (\n",
    "        translate_text(translator_lm, topic_raw, target_lang=\"en\", source_lang_hint=\"ko\")\n",
    "        if user_lang == \"ko\"\n",
    "        else topic_raw\n",
    "    )\n",
    "    runner_argument = RunnerArgument(\n",
    "        topic=topic,\n",
    "        language=user_lang,\n",
    "        retrieve_top_k=args.retrieve_top_k,\n",
    "        max_search_queries=args.max_search_queries,\n",
    "        total_conv_turn=args.total_conv_turn,\n",
    "        max_search_thread=args.max_search_thread,\n",
    "        max_search_queries_per_turn=args.max_search_queries_per_turn,\n",
    "        warmstart_max_num_experts=args.warmstart_max_num_experts,\n",
    "        warmstart_max_turn_per_experts=args.warmstart_max_turn_per_experts,\n",
    "        warmstart_max_thread=args.warmstart_max_thread,\n",
    "        max_thread_num=args.max_thread_num,\n",
    "        max_num_round_table_experts=args.max_num_round_table_experts,\n",
    "        moderator_override_N_consecutive_answering_turn=args.moderator_override_N_consecutive_answering_turn,\n",
    "        node_expansion_trigger_count=args.node_expansion_trigger_count,\n",
    "    )\n",
    "    logging_wrapper = LoggingWrapper(lm_config)\n",
    "    callback_handler = (\n",
    "        LocalConsolePrintCallBackHandler() if args.enable_log_print else None\n",
    "    )\n",
    "\n",
    "    # Co-STORM is a knowledge curation system which consumes information from the retrieval module.\n",
    "    # Currently, the information source is the Internet and we use search engine API as the retrieval module.\n",
    "    match args.retriever:\n",
    "        case \"bing\":\n",
    "            rm = BingSearch(\n",
    "                bing_search_api=os.getenv(\"BING_SEARCH_API_KEY\"),\n",
    "                k=runner_argument.retrieve_top_k,\n",
    "            )\n",
    "        case \"you\":\n",
    "            rm = YouRM(\n",
    "                ydc_api_key=os.getenv(\"YDC_API_KEY\"), k=runner_argument.retrieve_top_k\n",
    "            )\n",
    "        case \"brave\":\n",
    "            rm = BraveRM(\n",
    "                brave_search_api_key=os.getenv(\"BRAVE_API_KEY\"),\n",
    "                k=runner_argument.retrieve_top_k,\n",
    "            )\n",
    "        case \"duckduckgo\":\n",
    "            rm = DuckDuckGoSearchRM(\n",
    "                k=runner_argument.retrieve_top_k, safe_search=\"On\", region=\"us-en\"\n",
    "            )\n",
    "        case \"serper\":\n",
    "            rm = SerperRM(\n",
    "                serper_search_api_key=os.getenv(\"SERPER_API_KEY\"),\n",
    "                query_params={\"autocorrect\": True, \"num\": 10, \"page\": 1},\n",
    "            )\n",
    "        case \"tavily\":\n",
    "            rm = TavilySearchRM(\n",
    "                tavily_search_api_key=os.getenv(\"TAVILY_API_KEY\"),\n",
    "                k=runner_argument.retrieve_top_k,\n",
    "                include_raw_content=True,\n",
    "            )\n",
    "        case \"searxng\":\n",
    "            rm = SearXNG(\n",
    "                searxng_api_key=os.getenv(\"SEARXNG_API_KEY\"),\n",
    "                k=runner_argument.retrieve_top_k,\n",
    "            )\n",
    "        case _:\n",
    "            raise ValueError(\n",
    "                f'Invalid retriever: {args.retriever}. Choose either \"bing\", \"you\", \"brave\", \"duckduckgo\", \"serper\", \"tavily\", or \"searxng\"'\n",
    "            )\n",
    "\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    costorm_runner = CoStormRunner(\n",
    "        lm_config=lm_config,\n",
    "        runner_argument=runner_argument,\n",
    "        logging_wrapper=logging_wrapper,\n",
    "        rm=rm,\n",
    "        encoder=encoder,\n",
    "        callback_handler=callback_handler,\n",
    "    )\n",
    "\n",
    "    article = None\n",
    "    instance_copy = None\n",
    "    log_dump = None\n",
    "    error_payload = None\n",
    "    error_exc = None\n",
    "\n",
    "    try:\n",
    "        # warm start the system\n",
    "        costorm_runner.warm_start()\n",
    "\n",
    "        # Below is an example of how users may interact with Co-STORM to seek information together\n",
    "        # In actual deployment, we suggest allowing the user to decide whether to observe the agent utterance or inject a turn\n",
    "\n",
    "        # observing Co-STORM LLM agent utterance for 5 turns\n",
    "        for _ in range(1):\n",
    "            conv_turn = costorm_runner.step()\n",
    "            utter_to_show = (\n",
    "                translate_text(translator_lm, conv_turn.utterance, target_lang=\"ko\")\n",
    "                if user_lang == \"ko\"\n",
    "                else conv_turn.utterance\n",
    "            )\n",
    "            print(f\"**{conv_turn.role}**: {utter_to_show}\\n\")\n",
    "\n",
    "        # active engaging by injecting your utterance\n",
    "        your_utterance = input(\"Your utterance: \")\n",
    "        if user_lang == \"ko\":\n",
    "            your_utterance = translate_text(\n",
    "                translator_lm, your_utterance, target_lang=\"en\", source_lang_hint=\"ko\"\n",
    "            )\n",
    "        costorm_runner.step(user_utterance=your_utterance)\n",
    "\n",
    "        # continue observing\n",
    "        conv_turn = costorm_runner.step()\n",
    "        utter_to_show = (\n",
    "            translate_text(translator_lm, conv_turn.utterance, target_lang=\"ko\")\n",
    "            if user_lang == \"ko\"\n",
    "            else conv_turn.utterance\n",
    "        )\n",
    "        print(f\"**{conv_turn.role}**: {utter_to_show}\\n\")\n",
    "\n",
    "        # generate report\n",
    "        costorm_runner.knowledge_base.reorganize()\n",
    "        article = costorm_runner.generate_report()\n",
    "    except Exception as exc:\n",
    "        error_payload = {\n",
    "            \"error\": str(exc),\n",
    "            \"traceback\": traceback.format_exc(),\n",
    "        }\n",
    "        print(f\"Run failed: {exc}\")\n",
    "        error_exc = exc\n",
    "    finally:\n",
    "        try:\n",
    "            instance_copy = costorm_runner.to_dict()\n",
    "        except Exception as e:\n",
    "            instance_copy = instance_copy or {\"error\": f\"instance_dump_failed: {e}\"}\n",
    "        try:\n",
    "            log_dump = costorm_runner.dump_logging_and_reset()\n",
    "        except Exception as e:\n",
    "            log_dump = log_dump or {\"error\": f\"log_dump_failed: {e}\"}\n",
    "\n",
    "        # Save artifacts if available\n",
    "        if article is not None:\n",
    "            # If the pipeline ran in Korean, the generated article is already Korean.\n",
    "            if user_lang == \"ko\":\n",
    "                article_kr = article\n",
    "                with open(os.path.join(args.output_dir, \"report_kr.md\"), \"w\") as f:\n",
    "                    f.write(article_kr)\n",
    "            else:\n",
    "                # Default: English generation\n",
    "                with open(os.path.join(args.output_dir, \"report_eng.md\"), \"w\") as f:\n",
    "                    f.write(article)\n",
    "\n",
    "        if instance_copy is not None:\n",
    "            with open(os.path.join(args.output_dir, \"instance_dump.json\"), \"w\") as f:\n",
    "                json.dump(instance_copy, f, indent=2)\n",
    "\n",
    "        if log_dump is not None:\n",
    "            # Attach run configuration to help post-run analysis (including language).\n",
    "            try:\n",
    "                log_dump[\"runner_argument\"] = runner_argument.to_dict()\n",
    "            except Exception:\n",
    "                pass\n",
    "            with open(os.path.join(args.output_dir, \"log.json\"), \"w\") as f:\n",
    "                json.dump(log_dump, f, indent=2)\n",
    "\n",
    "        if error_payload is not None:\n",
    "            with open(os.path.join(args.output_dir, \"error.json\"), \"w\") as f:\n",
    "                json.dump(error_payload, f, indent=2)\n",
    "            # Re-raise to surface failure after saving artifacts\n",
    "            if error_exc is not None:\n",
    "                raise error_exc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27ca7e0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentence_transformers.SentenceTransformer : INFO     : Load pretrained SentenceTransformer: /data/models/nvidia-llama-embed-nemotron-8b\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:03<00:00,  1.10it/s]\n",
      "sentence_transformers.SentenceTransformer : INFO     : 1 prompt is loaded, with the key: query\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Topic:  거대 언어 모델에 대해서 설명해줘. 기술적인 관점에 초점을 맞춰줘.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m08:10:46 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Explain ... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:10:48 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Explain ... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m08:10:48 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm start update: Start getting familiar with the topic by chatting with multiple LLM experts (Step 1 / 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Topic co... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:10:52 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Topic co... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m08:10:54 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Large la... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:11:05 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Large la... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m08:11:06 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='1. Trans... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:11:12 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='1. Trans... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m08:11:12 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:11:12 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:11:12 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Topic fo... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:11:16 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Topic fo... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m08:11:17 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:11:19 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:11:20 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='', role=... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:11:23 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='', role=... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m08:11:23 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:11:27 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:11:29 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:11:32 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:11:34 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:11:34 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:11:34 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Large‑... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:11:46 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Large‑... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m08:11:47 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm start update: Finish browsing https://www.truefoundry.com/blog/transformer-architecture\n",
      "Finish browsing https://rpradeepmenon.medium.com/introduction-to-large-language-models-and-the-transformer-architecture-534408ed7e61\n",
      "Finish browsing https://www.datacamp.com/tutorial/how-transformers-work\n",
      "Finish browsing https://huggingface.co/learn/llm-course/en/chapter1/6\n",
      "Finish browsing https://en.wikipedia.org/wiki/Transformer_(deep_learning)\n",
      "Finish browsing https://www.emergentmind.com/topics/chinchilla-scaling-law\n",
      "Finish browsing https://lifearchitect.ai/chinchilla/\n",
      "Finish browsing https://medium.com/@raniahossam/chinchilla-scaling-laws-for-large-language-models-llms-40c434e4e1c1\n",
      "Finish browsing https://arxiv.org/abs/2001.08361\n",
      "Finish browsing https://aclanthology.org/2025.acl-long.1163.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='In a Tra... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:11:58 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='In a Tra... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m08:11:59 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm start update: Finish browsing https://stats.stackexchange.com/questions/421935/what-exactly-are-keys-queries-and-values-in-attention-mechanisms\n",
      "Finish browsing https://d2l.ai/chapter_attention-mechanisms-and-transformers/queries-keys-values.html\n",
      "Finish browsing https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html\n",
      "Finish browsing https://rahulrajpvr7d.medium.com/what-are-the-query-key-and-value-vectors-5656b8ca5fa0\n",
      "Finish browsing https://www.reddit.com/r/learnmachinelearning/comments/1bmu821/query_key_and_value_in_transformers/\n",
      "Finish browsing https://medium.com/@nachozobian/a-beginners-guide-to-self-attention-in-transformers-baf71a971efd\n",
      "Finish browsing https://www.codecademy.com/article/transformer-architecture-self-attention-mechanism\n",
      "Finish browsing https://towardsdatascience.com/contextual-transformer-embeddings-using-self-attention-explained-with-diagrams-and-python-code-d7a9f0f4d94e/\n",
      "Finish browsing https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html\n",
      "Finish browsing https://www.youtube.com/watch?v=eMlx5fFNoYc&vl=en\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m08:12:03 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:12:06 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:12:07 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:12:07 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:12:10 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:12:11 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:12:19 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:12:20 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm start update: Finish browsing https://venturebeat.com/ai/a-look-under-the-hood-of-transfomers-the-engine-driving-ai-model-evolution\n",
      "Finish browsing https://medium.com/@tfugislamabad/enhancing-large-language-models-through-lora-ml-paper-reading-clubs-day-1-83f1e742e386\n",
      "Finish browsing https://www.henryharvin.com/blog/large-language-models/\n",
      "Finish browsing https://www.extentia.com/why-are-large-language-models-becoming-important-to-businesses/\n",
      "Finish browsing https://aiwithmike.substack.com/p/a-route-to-large-language-models\n",
      "Finish browsing https://aws.amazon.com/what-is/large-language-model/\n",
      "Finish browsing https://blog.leena.ai/large-language-models-llms-guide/\n",
      "Finish browsing https://www.artiba.org/blog/how-do-large-language-models-work-how-to-train-them\n",
      "Finish browsing https://www.lakera.ai/blog/large-language-models-guide\n",
      "Finish browsing https://www.anyoneai.com/blog/how-are-large-language-models-trained-a-step-by-step-guide-to-llm-training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m08:12:25 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:12:28 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:12:32 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:12:36 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:12:36 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:12:36 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Scaling ... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:12:48 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Scaling ... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm start update: Finish browsing https://machinelearningmastery.com/training-a-model-on-multiple-gpus-with-data-parallelism/\n",
      "Finish browsing https://www.jeremyjordan.me/distributed-training/\n",
      "Finish browsing https://lilianweng.github.io/posts/2021-09-25-train-large/\n",
      "Finish browsing https://huggingface.co/docs/transformers/en/perf_train_gpu_many\n",
      "Finish browsing https://medium.com/@turbopython/how-does-llm-training-scale-to-over-10-000-gpus-the-4d-parallelism-you-need-to-know-now-4dcef620d2cb\n",
      "Finish browsing https://medium.com/@yananchen1116/a-rapid-guide-about-llms-training-in-parallelism-d6edf0dba876\n",
      "Finish browsing https://courses.cs.washington.edu/courses/cse599k/24au/content/parallelism.pdf\n",
      "Finish browsing https://www.genesiscloud.com/blog/top-parallelism-techniques-llm-training\n",
      "Finish browsing https://martynassubonis.substack.com/p/model-and-pipeline-parallelism\n",
      "Finish browsing https://discuss.huggingface.co/t/model-parallelism-and-pipelining-for-model-training/80084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m08:12:55 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm start update: Finish browsing https://www.wordreference.com/definition/large\n",
      "Finish browsing https://www.dictionary.com/browse/large\n",
      "Finish browsing https://www.definitions.net/definition/Large\n",
      "Finish browsing https://www.thefreedictionary.com/Large\n",
      "Finish browsing https://dictionary.cambridge.org/dictionary/english/large\n",
      "Finish browsing https://en.wikipedia.org/wiki/Scaling\n",
      "Finish browsing https://www.vocabulary.com/dictionary/scaling\n",
      "Finish browsing https://www.wordreference.com/enit/scaling\n",
      "Finish browsing https://en.wiktionary.org/wiki/scaling\n",
      "Finish browsing https://www.thefreedictionary.com/scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m08:13:01 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:13:04 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:13:17 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:13:17 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm start update: Finish browsing https://dl.acm.org/doi/full/10.1145/3744746\n",
      "Finish browsing https://arxiv.org/pdf/2307.06435\n",
      "Finish browsing https://resources.nvidia.com/en-us-new-llm-product/large-language-models-overview-web-page\n",
      "Finish browsing https://ashishjaiman.medium.com/large-language-models-llms-260bf4f39007\n",
      "Finish browsing https://www.ibm.com/think/topics/large-language-models\n",
      "Finish browsing https://jananithinks.medium.com/transformer-architecture-the-backbone-of-llms-1a3d085ca981\n",
      "Finish browsing https://www.leximancer.com/blog/snudvf8nn4omx6bmr3iprqarw8iyqf\n",
      "Finish browsing https://www.truefoundry.com/blog/transformer-architecture\n",
      "Finish browsing https://www.datacamp.com/tutorial/how-transformers-work\n",
      "Finish browsing https://aws.amazon.com/what-is/transformers-in-artificial-intelligence/\n",
      "Warm start update: Organizing collected information (Step 2 / 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='# Overvi... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:13:23 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='# Overvi... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m08:13:24 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:13:30 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:13:31 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:13:34 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:13:35 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:13:35 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm start update: Inserting collected information into knowledge base (Step 3 / 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='No reaso... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:13:36 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='No reaso... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m08:13:37 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:13:37 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:13:37 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='create: ... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:13:40 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='create: ... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m08:13:40 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:13:42 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:13:42 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:13:44 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:13:45 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:13:45 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:13:45 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:13:47 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:13:47 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:13:48 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:13:48 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:13:50 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:13:50 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Reasonin... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:13:53 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Reasonin... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m08:13:54 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:13:54 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:13:54 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:13:56 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:13:57 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:13:57 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:13:58 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:13:59 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:14:00 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:14:01 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:14:01 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:14:03 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:14:05 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:14:05 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:14:05 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:14:07 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:14:07 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:14:08 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:14:08 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:14:10 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:14:10 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content=\"Reasonin... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:14:12 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content=\"Reasonin... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/data/coscientist/storm/knowledge_storm/collaborative_storm/modules/information_insertion_module.py\", line 249, in process_intent\n",
      "    candidate_placement = self.layer_by_layer_navigation_placement(\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/coscientist/storm/knowledge_storm/collaborative_storm/modules/information_insertion_module.py\", line 135, in layer_by_layer_navigation_placement\n",
      "    raise ValueError(f\"Child node with name {node_name} not found.\")\n",
      "ValueError: Child node with name Machine Learning not found.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m08:14:13 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:14:14 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:14:14 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:14:15 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:14:16 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:14:17 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:14:17 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:14:18 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:14:19 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:14:20 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:14:21 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:14:21 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:14:23 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:14:23 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:14:23 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:14:25 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:14:25 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:14:26 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:14:26 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:14:28 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:14:28 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:14:31 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:14:32 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:14:33 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:14:34 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:14:34 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm start update: Synthesizing background information discussion utterances (Step 4 / 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='대형 ... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:14:49 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='대형 ... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m08:14:50 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Transfor... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:14:55 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Transfor... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m08:14:57 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:14:57 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:14:57 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:14:57 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:14:57 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Best pla... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:15:00 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Best pla... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m08:15:00 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Best pla... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Best pla... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:15:02 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:15:02 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Best pla... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Best pla... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:15:03 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:15:04 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Best pla... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Best pla... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:15:06 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:15:07 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Best pla... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Best pla... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:15:08 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:15:09 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Best pla... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Best pla... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:15:11 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:15:11 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Best pla... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Best pla... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:15:12 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:15:15 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:15:16 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:15:19 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:15:21 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:15:21 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Multi‑... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:15:26 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Multi‑... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m08:15:27 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:15:27 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:15:27 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:15:27 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:15:27 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Best pla... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:15:31 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Best pla... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m08:15:31 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Best pla... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Best pla... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:15:32 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:15:34 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:15:36 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:15:38 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:15:41 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start planning next expert; inspect mind map; inspect system state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m08:16:52 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='**Roundt... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:16:57 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='**Roundt... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m08:16:58 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='How do e... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:17:04 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='How do e... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m08:17:04 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='That ove... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:17:08 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='That ove... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start inserting information into mind map.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m08:17:09 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='That ove... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Best pla... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:17:12 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Best pla... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m08:17:13 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Best pla... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Best pla... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:17:14 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:17:15 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish inserting information into mind map.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='그 개... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:17:20 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Moderator**: 그 개요를 통해 현재 모델들이 얼마나 거대한지 명확히 알 수 있습니다. 이를 바탕으로 궁금한 점은, 모델 크기, 데이터 양, 그리고 연산량 사이의 관계를 정량화한 새로운 스케일링‑법칙 인사이트[33]가 등장함에 따라 연구자들은 핵심 트랜스포머 구성 요소—특히 멀티‑헤드 셀프‑어텐션과 피드‑포워드 레이어—를 어떻게 재설계하여 파라미터 수와 계산 비용을 줄이면서도 비슷한 성능을 유지하고 있는가 하는 것입니다[32].\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='그 개... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Your utterance:  ok that's a good point. go ahead survey.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m08:17:56 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Okay, th... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:17:58 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Okay, th... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='**Roundt... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m08:17:59 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start planning next expert; inspect mind map; inspect system state.\n",
      "Reviewing discourse history; Deciding utterance intent.\n",
      "Start searching with the search engine; browsing collected information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Topic co... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:18:03 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Topic co... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m08:18:05 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish browsing https://ieeexplore.ieee.org/document/10680313/\n",
      "Finish browsing https://www.preprints.org/manuscript/202508.0744\n",
      "Finish browsing https://link.springer.com/article/10.1007/s10462-024-10888-y\n",
      "Finish browsing https://www.mdpi.com/2079-9292/14/18/3580\n",
      "Finish browsing https://arxiv.org/abs/2402.06196\n",
      "Finish browsing https://medium.com/@roberto.g.infante/an-intuitive-overview-of-the-transformer-architecture-6a88ccc88171\n",
      "Finish browsing https://huggingface.co/learn/llm-course/en/chapter1/6\n",
      "Finish browsing https://www.truefoundry.com/blog/transformer-architecture\n",
      "Finish browsing https://www.geeksforgeeks.org/nlp/large-language-models-llms-vs-transformers/\n",
      "Finish browsing https://www.datacamp.com/tutorial/how-transformers-work\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Large La... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:18:15 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Large La... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m08:18:16 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish generating utterance from collected information.\n",
      "Start polishing utterance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Sure, le... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:18:24 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Sure, le... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m08:18:24 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='1. Acade... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:18:28 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='1. Acade... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m08:18:29 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:18:32 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:18:32 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start inserting information into mind map.\n",
      "Finish inserting information into mind map.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='물론, ... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:18:39 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='물론, ... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Multi‑... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**General Knowledge Provider**: 물론, 오늘날 이 분야가 어떤 모습인지 간단히 정리해 드리겠습니다.\n",
      "\n",
      "**대형 언어 모델(Large Language Models, LLMs)** 은 본질적으로 **Transformer 기반 신경망** 으로, 수십억~수조 개의 토큰으로 이루어진 방대한 코퍼스를 학습해 텍스트를 이해하고 생성합니다【6】. Transformer의 핵심 기술은 **멀티‑헤드 셀프‑어텐션** 으로, 각 토큰이 다른 모든 토큰의 맥락을 포착하게 하고, 이후 피드‑포워드 레이어가 이러한 표현을 정제\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Best pla... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Best pla... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Best pla... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Best pla... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Best pla... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Best pla... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m08:18:40 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:18:40 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:18:40 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:18:40 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Transfor... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:18:52 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Transfor... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m08:19:01 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:19:12 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='### Scal... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:19:24 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='### Scal... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Edit this list to match your desired CLI args.\n",
    "arg_list = [\n",
    "    \"--retriever\", \"tavily\",\n",
    "    \"--llm-provider\", \"ollama\",\n",
    "    \"--llm-model\", \"gpt-oss:120b\",\n",
    "    \"--llm-url\", \"http://localhost\",\n",
    "    \"--llm-port\", \"11434\",\n",
    "    \"--encoder-type\", \"hf_local\",\n",
    "    \"--embedding-model\", \"/data/models/nvidia-llama-embed-nemotron-8b\",\n",
    "    \"--secrets-file\", \"/data/coscientist/secrets.toml\",\n",
    "    \"--output-dir\", \"./results/co-storm-notebook\",\n",
    "    \"--enable_log_print\",\n",
    "]\n",
    "\n",
    "parser = ArgumentParser()\n",
    "parser.add_argument(\"--output-dir\", type=str, default=\"./results/co-storm\", help=\"Directory to store the outputs.\")\n",
    "parser.add_argument(\"--llm-provider\", type=str, choices=[\"ollama\", \"openai\", \"azure\"], default=\"ollama\", help=\"LLM provider to use.\")\n",
    "parser.add_argument(\"--llm-model\", type=str, default=\"gpt-oss:120b\", help=\"Model name for the selected LLM provider (for Ollama, omit the 'ollama/' prefix).\")\n",
    "parser.add_argument(\"--llm-url\", type=str, default=\"http://localhost\", help=\"Base URL for the LLM service (used for Ollama).\")\n",
    "parser.add_argument(\"--llm-port\", type=int, default=11434, help=\"Port for the LLM service (used for Ollama).\")\n",
    "parser.add_argument(\"--ollama-model-dir\", type=str, default=\"/data/ollama/models\", help=\"Directory where Ollama should store models.\")\n",
    "parser.add_argument(\"--llm-temperature\", type=float, default=1.0, help=\"Sampling temperature for the LLM.\")\n",
    "parser.add_argument(\"--llm-top-p\", type=float, default=0.9, help=\"Top-p for nucleus sampling.\")\n",
    "parser.add_argument(\"--encoder-type\", type=str, choices=[\"hf_local\", \"ollama\", \"openai\", \"azure\"], default=\"hf_local\", help=\"Embedding backend to use.\")\n",
    "parser.add_argument(\"--embedding-model\", type=str, default=\"/data/models/nvidia-llama-embed-nemotron-8b\", help=\"Embedding model name or local path.\")\n",
    "parser.add_argument(\"--embedding-base-url\", type=str, default=\"http://localhost\", help=\"Base URL for embedding service when encoder-type is ollama.\")\n",
    "parser.add_argument(\"--embedding-port\", type=int, default=11434, help=\"Port for embedding service when encoder-type is ollama.\")\n",
    "parser.add_argument(\"--embedding-device\", type=str, default=\"auto\", help=\"Device for local embeddings (auto, cpu, cuda).\")\n",
    "parser.add_argument(\"--embedding-cache-dir\", type=str, default=\"/data/models\", help=\"Cache directory / HF_HOME for local embedding models.\")\n",
    "parser.add_argument(\"--secrets-file\", type=str, default=\"/data/coscientist/secrets.toml\", help=\"Path to secrets.toml for API keys.\")\n",
    "parser.add_argument(\"--retriever\", type=str, choices=[\"bing\", \"you\", \"brave\", \"serper\", \"duckduckgo\", \"tavily\", \"searxng\"], default=\"duckduckgo\", help=\"The search engine API to use for retrieving information.\")\n",
    "parser.add_argument(\"--retrieve_top_k\", type=int, default=10, help=\"Retrieve top k results for each query in retriever.\")\n",
    "parser.add_argument(\"--max_search_queries\", type=int, default=2, help=\"Maximum number of search queries to consider for each question.\")\n",
    "parser.add_argument(\"--total_conv_turn\", type=int, default=20, help=\"Maximum number of turns in conversation.\")\n",
    "parser.add_argument(\"--max_search_thread\", type=int, default=5, help=\"Maximum number of parallel threads for retriever.\")\n",
    "parser.add_argument(\"--max_search_queries_per_turn\", type=int, default=3, help=\"Maximum number of search queries to consider in each turn.\")\n",
    "parser.add_argument(\"--warmstart_max_num_experts\", type=int, default=3, help=\"Max number of experts in perspective-guided QA during warm start.\")\n",
    "parser.add_argument(\"--warmstart_max_turn_per_experts\", type=int, default=2, help=\"Max number of turns per perspective during warm start.\")\n",
    "parser.add_argument(\"--warmstart_max_thread\", type=int, default=3, help=\"Max number of threads for parallel perspective-guided QA during warm start.\")\n",
    "parser.add_argument(\"--max_thread_num\", type=int, default=10, help=\"Maximum number of threads to use.\")\n",
    "parser.add_argument(\"--max_num_round_table_experts\", type=int, default=2, help=\"Max number of active experts in round table discussion.\")\n",
    "parser.add_argument(\"--moderator_override_N_consecutive_answering_turn\", type=int, default=3, help=\"Number of consecutive expert answering turns before the moderator overrides the conversation.\")\n",
    "parser.add_argument(\"--node_expansion_trigger_count\", type=int, default=10, help=\"Trigger node expansion for nodes that contain more than N snippets.\")\n",
    "parser.add_argument(\"--enable_log_print\", action=\"store_true\", help=\"If set, enable console log print.\")\n",
    "\n",
    "args = parser.parse_args(arg_list)\n",
    "main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d73426-af58-4adb-91ca-d69d30b9f653",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (coscientist)",
   "language": "python",
   "name": "coscientist"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
