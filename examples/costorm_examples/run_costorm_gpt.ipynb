{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3c290f5",
   "metadata": {},
   "source": [
    "# Co-STORM (Ollama + Local Embeddings)\n",
    "\n",
    "This notebook mirrors `examples/costorm_examples/run_costorm_gpt.py` so you can run Co-STORM from a notebook while keeping the same behavior (Ollama LLM, local Nemotron embeddings, artifact saving). Edit the argument list in the last cell to fit your environment (model paths, retriever keys, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b3bfdbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import traceback\n",
    "from argparse import ArgumentParser\n",
    "from typing import Optional\n",
    "from pathlib import Path\n",
    "import importlib\n",
    "\n",
    "# Force repo root for imports so we use the local knowledge_storm implementation.\n",
    "REPO_ROOT = Path('/data/coscientist/storm')\n",
    "if str(REPO_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(REPO_ROOT))\n",
    "# Also set PYTHONPATH for any child processes\n",
    "os.environ.setdefault('PYTHONPATH', str(REPO_ROOT))\n",
    "\n",
    "# Purge any previously imported knowledge_storm modules to force re-import from REPO_ROOT\n",
    "for mod in list(sys.modules.keys()):\n",
    "    if mod.startswith('knowledge_storm'):\n",
    "        sys.modules.pop(mod)\n",
    "\n",
    "from knowledge_storm.collaborative_storm.engine import (\n",
    "    CollaborativeStormLMConfigs,\n",
    "    RunnerArgument,\n",
    "    CoStormRunner,\n",
    ")\n",
    "from knowledge_storm.collaborative_storm.modules.callback import (\n",
    "    LocalConsolePrintCallBackHandler,\n",
    ")\n",
    "from knowledge_storm.lm import LitellmModel, OpenAIModel, AzureOpenAIModel\n",
    "from knowledge_storm.logging_wrapper import LoggingWrapper\n",
    "from knowledge_storm.rm import (\n",
    "    YouRM,\n",
    "    BingSearch,\n",
    "    BraveRM,\n",
    "    SerperRM,\n",
    "    DuckDuckGoSearchRM,\n",
    "    TavilySearchRM,\n",
    "    SearXNG,\n",
    ")\n",
    "from knowledge_storm.encoder import Encoder\n",
    "from knowledge_storm.utils import load_api_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec8073ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_base_url(url: str, port: Optional[int] = None) -> str:\n",
    "    \"\"\"Normalize base URL and optionally append port.\"\"\"\n",
    "    if not url.startswith(\"http://\") and not url.startswith(\"https://\"):\n",
    "        url = f\"http://{url}\"\n",
    "    url = url.rstrip(\"/\")\n",
    "    if port and f\":{port}\" not in url.split(\"//\", 1)[-1]:\n",
    "        url = f\"{url}:{port}\"\n",
    "    return url\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    load_api_key(toml_file_path=args.secrets_file)\n",
    "    lm_config: CollaborativeStormLMConfigs = CollaborativeStormLMConfigs()\n",
    "    if args.llm_provider == \"ollama\" and args.ollama_model_dir:\n",
    "        os.environ.setdefault(\"OLLAMA_MODELS\", args.ollama_model_dir)\n",
    "\n",
    "    if args.encoder_type == \"hf_local\" and args.embedding_cache_dir:\n",
    "        os.environ.setdefault(\"HF_HOME\", args.embedding_cache_dir)\n",
    "\n",
    "    embedding_base_url = (\n",
    "        build_base_url(args.embedding_base_url, args.embedding_port)\n",
    "        if args.encoder_type == \"ollama\"\n",
    "        else None\n",
    "    )\n",
    "    encoder_device = None if args.embedding_device == \"auto\" else args.embedding_device\n",
    "    # Pass model via env for compatibility with older Encoder versions\n",
    "    if args.embedding_model:\n",
    "        os.environ[\"ENCODER_MODEL_NAME\"] = args.embedding_model\n",
    "    encoder = Encoder(\n",
    "        encoder_type=args.encoder_type,\n",
    "        api_base=embedding_base_url,\n",
    "    )\n",
    "\n",
    "    llm_provider = args.llm_provider.lower()\n",
    "    if llm_provider == \"ollama\":\n",
    "        llm_base_url = build_base_url(args.llm_url, args.llm_port)\n",
    "        model_name = args.llm_model\n",
    "        if not model_name.startswith(\"ollama/\"):\n",
    "            model_name = f\"ollama/{model_name}\"\n",
    "        ollama_kwargs = {\n",
    "            \"base_url\": llm_base_url,\n",
    "            \"temperature\": args.llm_temperature,\n",
    "            \"top_p\": args.llm_top_p,\n",
    "            \"model_type\": \"chat\",\n",
    "        }\n",
    "\n",
    "        def build_lm(max_tokens: int):\n",
    "            return LitellmModel(\n",
    "                model=model_name,\n",
    "                max_tokens=max_tokens,\n",
    "                **ollama_kwargs,\n",
    "            )\n",
    "\n",
    "    elif llm_provider == \"openai\":\n",
    "        openai_kwargs = {\n",
    "            \"api_key\": os.getenv(\"OPENAI_API_KEY\"),\n",
    "            \"api_provider\": \"openai\",\n",
    "            \"temperature\": args.llm_temperature,\n",
    "            \"top_p\": args.llm_top_p,\n",
    "            \"api_base\": None,\n",
    "        }\n",
    "        ModelClass = OpenAIModel\n",
    "        gpt_4o_model_name = \"gpt-4o\"\n",
    "\n",
    "        def build_lm(max_tokens: int):\n",
    "            return ModelClass(\n",
    "                model=gpt_4o_model_name, max_tokens=max_tokens, **openai_kwargs\n",
    "            )\n",
    "\n",
    "    elif llm_provider == \"azure\":\n",
    "        openai_kwargs = {\n",
    "            \"api_key\": os.getenv(\"AZURE_API_KEY\"),\n",
    "            \"temperature\": args.llm_temperature,\n",
    "            \"top_p\": args.llm_top_p,\n",
    "            \"api_base\": os.getenv(\"AZURE_API_BASE\"),\n",
    "            \"api_version\": os.getenv(\"AZURE_API_VERSION\"),\n",
    "        }\n",
    "        ModelClass = AzureOpenAIModel\n",
    "        gpt_4o_model_name = \"gpt-4o\"\n",
    "\n",
    "        def build_lm(max_tokens: int):\n",
    "            return ModelClass(\n",
    "                model=gpt_4o_model_name, max_tokens=max_tokens, **openai_kwargs\n",
    "            )\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f'Invalid llm provider: {args.llm_provider}. Choose either \"ollama\", \"openai\", or \"azure\".'\n",
    "        )\n",
    "\n",
    "    question_answering_lm = build_lm(1000)\n",
    "    discourse_manage_lm = build_lm(500)\n",
    "    utterance_polishing_lm = build_lm(2000)\n",
    "    warmstart_outline_gen_lm = build_lm(500)\n",
    "    question_asking_lm = build_lm(300)\n",
    "    knowledge_base_lm = build_lm(1000)\n",
    "\n",
    "    lm_config.set_question_answering_lm(question_answering_lm)\n",
    "    lm_config.set_discourse_manage_lm(discourse_manage_lm)\n",
    "    lm_config.set_utterance_polishing_lm(utterance_polishing_lm)\n",
    "    lm_config.set_warmstart_outline_gen_lm(warmstart_outline_gen_lm)\n",
    "    lm_config.set_question_asking_lm(question_asking_lm)\n",
    "    lm_config.set_knowledge_base_lm(knowledge_base_lm)\n",
    "\n",
    "    topic = input(\"Topic: \")\n",
    "    runner_argument = RunnerArgument(\n",
    "        topic=topic,\n",
    "        retrieve_top_k=args.retrieve_top_k,\n",
    "        max_search_queries=args.max_search_queries,\n",
    "        total_conv_turn=args.total_conv_turn,\n",
    "        max_search_thread=args.max_search_thread,\n",
    "        max_search_queries_per_turn=args.max_search_queries_per_turn,\n",
    "        warmstart_max_num_experts=args.warmstart_max_num_experts,\n",
    "        warmstart_max_turn_per_experts=args.warmstart_max_turn_per_experts,\n",
    "        warmstart_max_thread=args.warmstart_max_thread,\n",
    "        max_thread_num=args.max_thread_num,\n",
    "        max_num_round_table_experts=args.max_num_round_table_experts,\n",
    "        moderator_override_N_consecutive_answering_turn=args.moderator_override_N_consecutive_answering_turn,\n",
    "        node_expansion_trigger_count=args.node_expansion_trigger_count,\n",
    "    )\n",
    "    logging_wrapper = LoggingWrapper(lm_config)\n",
    "    callback_handler = (\n",
    "        LocalConsolePrintCallBackHandler() if args.enable_log_print else None\n",
    "    )\n",
    "\n",
    "    match args.retriever:\n",
    "        case \"bing\":\n",
    "            rm = BingSearch(\n",
    "                bing_search_api=os.getenv(\"BING_SEARCH_API_KEY\"),\n",
    "                k=runner_argument.retrieve_top_k,\n",
    "            )\n",
    "        case \"you\":\n",
    "            rm = YouRM(\n",
    "                ydc_api_key=os.getenv(\"YDC_API_KEY\"), k=runner_argument.retrieve_top_k\n",
    "            )\n",
    "        case \"brave\":\n",
    "            rm = BraveRM(\n",
    "                brave_search_api_key=os.getenv(\"BRAVE_API_KEY\"),\n",
    "                k=runner_argument.retrieve_top_k,\n",
    "            )\n",
    "        case \"duckduckgo\":\n",
    "            rm = DuckDuckGoSearchRM(\n",
    "                k=runner_argument.retrieve_top_k, safe_search=\"On\", region=\"us-en\"\n",
    "            )\n",
    "        case \"serper\":\n",
    "            rm = SerperRM(\n",
    "                serper_search_api_key=os.getenv(\"SERPER_API_KEY\"),\n",
    "                query_params={\"autocorrect\": True, \"num\": 10, \"page\": 1},\n",
    "            )\n",
    "        case \"tavily\":\n",
    "            rm = TavilySearchRM(\n",
    "                tavily_search_api_key=os.getenv(\"TAVILY_API_KEY\"),\n",
    "                k=runner_argument.retrieve_top_k,\n",
    "                include_raw_content=True,\n",
    "            )\n",
    "        case \"searxng\":\n",
    "            rm = SearXNG(\n",
    "                searxng_api_key=os.getenv(\"SEARXNG_API_KEY\"),\n",
    "                k=runner_argument.retrieve_top_k,\n",
    "            )\n",
    "        case _:\n",
    "            raise ValueError(\n",
    "                f'Invalid retriever: {args.retriever}. Choose either \"bing\", \"you\", \"brave\", \"duckduckgo\", \"serper\", \"tavily\", or \"searxng\"'\n",
    "            )\n",
    "\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    costorm_runner = CoStormRunner(\n",
    "        lm_config=lm_config,\n",
    "        runner_argument=runner_argument,\n",
    "        logging_wrapper=logging_wrapper,\n",
    "        rm=rm,\n",
    "        encoder=encoder,\n",
    "        callback_handler=callback_handler,\n",
    "    )\n",
    "\n",
    "    article = None\n",
    "    instance_copy = None\n",
    "    log_dump = None\n",
    "    error_payload = None\n",
    "    error_exc = None\n",
    "\n",
    "    try:\n",
    "        costorm_runner.warm_start()\n",
    "\n",
    "        for _ in range(1):\n",
    "            conv_turn = costorm_runner.step()\n",
    "            print(f\"**{conv_turn.role}**: {conv_turn.utterance}\\n\")\n",
    "\n",
    "        your_utterance = input(\"Your utterance: \")\n",
    "        costorm_runner.step(user_utterance=your_utterance)\n",
    "\n",
    "        conv_turn = costorm_runner.step()\n",
    "        print(f\"**{conv_turn.role}**: {conv_turn.utterance}\\n\")\n",
    "\n",
    "        costorm_runner.knowledge_base.reorganize()\n",
    "        article = costorm_runner.generate_report()\n",
    "    except Exception as exc:\n",
    "        error_payload = {\n",
    "            \"error\": str(exc),\n",
    "            \"traceback\": traceback.format_exc(),\n",
    "        }\n",
    "        print(f\"Run failed: {exc}\")\n",
    "        error_exc = exc\n",
    "    finally:\n",
    "        try:\n",
    "            instance_copy = costorm_runner.to_dict()\n",
    "        except Exception as e:\n",
    "            instance_copy = instance_copy or {\"error\": f\"instance_dump_failed: {e}\"}\n",
    "        try:\n",
    "            log_dump = costorm_runner.dump_logging_and_reset()\n",
    "        except Exception as e:\n",
    "            log_dump = log_dump or {\"error\": f\"log_dump_failed: {e}\"}\n",
    "\n",
    "        if article is not None:\n",
    "            with open(os.path.join(args.output_dir, \"report.md\"), \"w\") as f:\n",
    "                f.write(article)\n",
    "\n",
    "        if instance_copy is not None:\n",
    "            with open(os.path.join(args.output_dir, \"instance_dump.json\"), \"w\") as f:\n",
    "                json.dump(instance_copy, f, indent=2)\n",
    "\n",
    "        if log_dump is not None:\n",
    "            with open(os.path.join(args.output_dir, \"log.json\"), \"w\") as f:\n",
    "                json.dump(log_dump, f, indent=2)\n",
    "\n",
    "        if error_payload is not None:\n",
    "            with open(os.path.join(args.output_dir, \"error.json\"), \"w\") as f:\n",
    "                json.dump(error_payload, f, indent=2)\n",
    "            if error_exc is not None:\n",
    "                raise error_exc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b9e331f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentence_transformers.SentenceTransformer : INFO     : Load pretrained SentenceTransformer: /data/models/nvidia-llama-embed-nemotron-8b\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:01<00:00,  2.08it/s]\n",
      "sentence_transformers.SentenceTransformer : INFO     : 1 prompt is loaded, with the key: query\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Topic:  거대 언어모델에 대해서 설명해줘. 기술 특징에 대해서 조사해야해.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m08:39:46 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm start update: Start getting familiar with the topic by chatting with multiple LLM experts (Step 1 / 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Topic co... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:40:32 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Topic co... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m08:40:34 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='거대 ... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:40:47 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='거대 ... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m08:40:47 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='1. AI ... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:40:53 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='1. AI ... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m08:40:54 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:40:54 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:40:54 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Topic fo... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:40:58 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Topic fo... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m08:40:59 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:41:02 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:41:02 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:41:05 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:41:05 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:41:14 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:41:16 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:41:20 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='**Topic ... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:41:27 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='**Topic ... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m08:41:28 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing expert AI 연구자 (Transformer 및 모델 구조 전문가): 대규모 언어 모델의 핵심인 Transformer 아키텍처와 self‑attention 메커니즘, 파라미터 규모가 성능에 미치는 영향 등을 과학적 근거와 최신 연구 결과를 바탕으로 설명하고, 기술적 한계와 향후 발전 방향을 제시합니다.: Query is invalid.\n",
      "Error processing expert AI 연구자 (Transformer 및 모델 구조 전문가): 대규모 언어 모델의 핵심인 Transformer 아키텍처와 self‑attention 메커니즘, 파라미터 규모가 성능에 미치는 영향 등을 과학적 근거와 최신 연구 결과를 바탕으로 설명하고, 기술적 한계와 향후 발전 방향을 제시합니다.: Query is invalid.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='대규... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:41:40 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='대규... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m08:41:40 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm start update: Finish browsing https://aws.amazon.com/ko/blogs/tech/hyperaccel-fpga-on-aws/\n",
      "Finish browsing https://velog.io/@hbcho/LLM-%EC%B6%94%EB%A1%A0-%EB%B9%84%EC%9A%A9-%EA%B5%AC%EC%A1%B0-%EC%99%84%EB%B2%BD-%EC%9D%B4%ED%95%B4-%EC%9D%B8%ED%92%8B%EA%B3%BC-%EC%95%84%EC%9B%83%ED%92%8B%EC%9D%80-%EC%99%9C-%EB%8B%A4%EB%A5%B4%EA%B2%8C-%EA%B3%BC%EA%B8%88%EB%90%A0%EA%B9%8C\n",
      "Finish browsing https://blog.scatterlab.co.kr/serving-architecture-3\n",
      "Finish browsing https://dytis.tistory.com/68\n",
      "Finish browsing https://www.reddit.com/r/LocalLLaMA/comments/1cqu4zf/cloud_gpus_for_llm_finetuning_storage_cost_seems/?tl=ko\n",
      "Finish browsing https://www.akamai.com/ko/glossary/what-is-a-large-language-model\n",
      "Finish browsing https://www.oreilly.com/library/view/daegyumo-eoneo-modeleul/0642572313845/ch05.html\n",
      "Finish browsing https://www.snowflake.com/ko/fundamentals/large-language-model/\n",
      "Finish browsing https://gptskorea.com/BLOG/?idx=93998061&bmode=view\n",
      "Finish browsing https://www.elastic.co/kr/what-is/large-language-models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m08:41:50 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:41:51 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm start update: Finish browsing https://culture.kookmin.ac.kr/gulmal/contests/1/applicants/179/attach\n",
      "Finish browsing http://www.unfuture.org/1528\n",
      "Finish browsing http://www.docdocdoc.co.kr/news/articleView.html?idxno=3024859\n",
      "Finish browsing https://www.genetec.com/ko/blog/cybersecurity/the-implications-of-large-language-models-in-physical-security\n",
      "Finish browsing https://blog.naver.com/wisestone2007/223398205862?viewType=pc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='', role=... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:41:54 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='', role=... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m08:41:54 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:41:57 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:41:58 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:41:59 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:42:00 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:42:01 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:42:02 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:42:08 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:42:11 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:42:14 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing expert 윤리·정책 전문가 (사회·법적 영향 분석가): 거대 언어 모델의 윤리적 위험성, 데이터 프라이버시, 편향 및 책임 문제를 조명하고, 규제 프레임워크와 기업·정부 차원의 책임 있는 AI 활용 방안을 제시합니다.: Query is invalid.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m08:42:26 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:42:26 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm start update: Finish browsing https://medium.com/@hugmanskj/%EA%B1%B0%EB%8C%80-%EC%96%B8%EC%96%B4-%EB%AA%A8%EB%8D%B8-large-language-model-%EC%97%90-%EB%8C%80%ED%95%9C-%EC%9D%B4%ED%95%B4-llm%EC%9D%98-%EA%B8%B0%EC%A4%80%EA%B3%BC-%ED%8A%B9%EC%A7%95%EC%9D%80-%EB%AC%B4%EC%97%87%EC%9D%BC%EA%B9%8C-0551b7b9d3bd\n",
      "Finish browsing https://aws.amazon.com/ko/what-is/large-language-model/\n",
      "Finish browsing https://www.lgresearch.ai/blog/view?seq=351\n",
      "Finish browsing https://blog.naver.com/jack0604/223476644379\n",
      "Finish browsing https://ko.wikipedia.org/wiki/%EB%8C%80%ED%98%95_%EC%96%B8%EC%96%B4_%EB%AA%A8%EB%8D%B8\n",
      "Finish browsing https://imasoftwareengineer.tistory.com/106\n",
      "Finish browsing https://medium.com/@hugmanskj/%EA%B1%B0%EB%8C%80-%EC%96%B8%EC%96%B4-%EB%AA%A8%EB%8D%B8-large-language-model-%EC%97%90-%EB%8C%80%ED%95%9C-%EC%9D%B4%ED%95%B4-llm%EC%9D%98-%EA%B8%B0%EC%A4%80%EA%B3%BC-%ED%8A%B9%EC%A7%95%EC%9D%80-%EB%AC%B4%EC%97%87%EC%9D%BC%EA%B9%8C-0551b7b9d3bd\n",
      "Finish browsing https://aws.amazon.com/ko/what-is/large-language-model/\n",
      "Finish browsing https://www.bureauworks.com/ko/blog/daegyumo-eoneo-modeli-mueosingayo\n",
      "Finish browsing https://www.elastic.co/kr/what-is/large-language-models\n",
      "Warm start update: Organizing collected information (Step 2 / 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Write th... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:42:33 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Write th... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m08:42:33 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='The topi... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:42:40 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='The topi... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm start update: Inserting collected information into knowledge base (Step 3 / 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m08:42:41 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:42:41 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Best pla... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:42:43 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Best pla... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m08:42:45 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:42:46 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Best pla... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='No reaso... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:42:48 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='No reaso... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m08:42:48 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='create: ... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:42:52 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='create: ... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m08:42:52 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:42:54 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:42:55 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='create: ... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='No reaso... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:42:58 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='No reaso... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m08:42:58 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='create: ... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:43:00 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:43:01 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:43:03 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:43:04 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:43:04 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='create: ... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Best pla... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:43:06 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Best pla... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m08:43:08 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:43:08 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:43:08 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:43:08 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm start update: Synthesizing background information discussion utterances (Step 4 / 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='대규... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:43:19 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='대규... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m08:43:24 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Large la... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:43:36 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Large la... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m08:43:37 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:43:37 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='\"우리 ... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:43:40 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='\"우리 ... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m08:43:40 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='**Questi... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:43:42 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='**Questi... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m08:43:42 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='거대 ... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:43:48 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='거대 ... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m08:43:51 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:43:51 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Definiti... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:43:55 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Definiti... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m08:43:56 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:43:56 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:43:56 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Best pla... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:43:58 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Best pla... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m08:44:00 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:44:01 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:44:01 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start planning next expert; inspect mind map; inspect system state.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='**Brief ... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:44:08 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='**Brief ... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m08:44:09 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='How are ... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:44:13 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='How are ... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m08:44:14 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Building... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:44:18 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start inserting information into mind map.\n",
      "Finish inserting information into mind map.\n",
      "**Moderator**: Building on that foundation, I’m curious—how are emerging tricks such as model sparsity, quantization, and retrieval‑augmented generation being used to trim inference costs while still preserving the performance gains we see with these\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Building... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Your utterance:  좋은 방향입니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m08:45:33 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start planning next expert; inspect mind map; inspect system state.\n",
      "Reviewing discourse history; Deciding utterance intent.\n",
      "Start searching with the search engine; browsing collected information.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Topic co... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:45:38 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Topic co... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m08:45:41 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish browsing https://www.snowflake.com/ko/fundamentals/large-language-model/\n",
      "Finish browsing https://ahha.ai/2024/07/17/llm/\n",
      "Finish browsing https://www.mfitlab.com/solutions/blog/llm\n",
      "Finish browsing https://www.bureauworks.com/ko/blog/daegyumo-eoneo-modeli-mueosingayo\n",
      "Finish browsing https://pmc.ncbi.nlm.nih.gov/articles/PMC11473987/\n",
      "Finish browsing https://medium.com/@hugmanskj/%EA%B1%B0%EB%8C%80-%EC%96%B8%EC%96%B4-%EB%AA%A8%EB%8D%B8-large-language-model-%EC%97%90-%EB%8C%80%ED%95%9C-%EC%9D%B4%ED%95%B4-llm%EC%9D%98-%EA%B8%B0%EC%A4%80%EA%B3%BC-%ED%8A%B9%EC%A7%95%EC%9D%80-%EB%AC%B4%EC%97%87%EC%9D%BC%EA%B9%8C-0551b7b9d3bd\n",
      "Finish browsing https://aws.amazon.com/ko/what-is/large-language-model/\n",
      "Finish browsing https://www.lgresearch.ai/blog/view?seq=351\n",
      "Finish browsing https://blog.naver.com/jack0604/223476644379\n",
      "Finish browsing https://ko.wikipedia.org/wiki/%EB%8C%80%ED%98%95_%EC%96%B8%EC%96%B4_%EB%AA%A8%EB%8D%B8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='거대 ... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:45:53 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='거대 ... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m08:45:53 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish generating utterance from collected information.\n",
      "Start polishing utterance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='맞아... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:46:01 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='맞아... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m08:46:01 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='1. 대... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:46:06 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='1. 대... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m08:46:07 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='', role=... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:46:10 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='', role=... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m08:46:11 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start inserting information into mind map.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='', role=... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Best pla... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:46:14 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Best pla... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m08:46:14 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Best pla... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Best pla... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:46:17 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:46:17 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish inserting information into mind map.\n",
      "**General Knowledge Provider**: 맞아요, 거대 언어모델(Large Language Model, LLM)은 수십 ~ 수조 파라미터와 수천억 ~ 수조 토큰 규모의 텍스트 코퍼스를 사전 학습해 다음 토큰을 예측함으로써 맥락·문맥을 깊이 이해하고 고급 추론까지 수행합니다[26][31].  \n",
      "\n",
      "- **전이·Few‑shot·Zero‑shot**: 사전 학습된 지식을 그대로 다양한 다운스트림 작업에 적용하고, 별도 미세조정 없이도 몇 개의 예시만으로 새로운 작업을 해결합니다[28].  \n",
      "- **다목적·유연성**: 하나의 모델이 질의응답, 요약, 번역, 문장 완성 등 서로 다른 작업을 동일 파라미터 집합으로 처리합니다[29].  \n",
      "- **규모**: 예를 들어 Google PaLM 2는 340 billion 파라미터·3.6 trillion 토큰, Meta LLaMA는 65 billion 파라미터·1.4 trillion 토큰으로 학습되었습니다[32].  \n",
      "\n",
      "**산업 적용**  \n",
      "- 데이터 사이언스 → 감성 분석·NLU·비구조화 데이터 구조화 등 자동화·고도화[23][25].  \n",
      "- 제조업 → 예측 유지보수·품질 관리·프로세스 최적화[24].  \n",
      "- 고객 서비스·교육·의료·창작 → 맞춤형 챗봇·자동 번역·콘텐츠 생성 등 혁신적 서비스 제공[26][27].  \n",
      "\n",
      "요약하면, LLM은 대규모 파라미터·토큰 기반 다중 작업 학습을 통해 문맥 이해·추론·전이 학습을 구현하고, 다양한 산업에서 업무 효율성과 혁신을 촉진하는 핵심 기술이라 할 수 있습니다[23][28][31].\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Scaling ... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:46:23 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Scaling ... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m08:46:24 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:46:24 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:46:24 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Best pla... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:46:27 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Best pla... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "\u001b[92m08:46:30 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:46:32 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m08:46:32 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:46:32 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "\u001b[92m08:46:32 - LiteLLM:INFO\u001b[0m: utils.py:3427 - \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "LiteLLM : INFO     : \n",
      "LiteLLM completion() model= gpt-oss:120b; provider = ollama\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Empirica... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:46:43 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Empirica... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='대규... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:46:53 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='대규... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:528: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='The back... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_json(\n",
      "\u001b[92m08:47:01 - LiteLLM:INFO\u001b[0m: utils.py:1307 - Wrapper: Completed Call, calling success_handler\n",
      "LiteLLM : INFO     : Wrapper: Completed Call, calling success_handler\n",
      "/home/ebert/miniconda3/envs/coscientist/lib/python3.11/site-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 6: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='The back... reasoning_content=None), input_type=Message])\n",
      "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...reasoning_content=None)), input_type=Choices])\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Edit this list to match your desired CLI args.\n",
    "arg_list = [\n",
    "    \"--retriever\", \"tavily\",\n",
    "    \"--llm-provider\", \"ollama\",\n",
    "    \"--llm-model\", \"gpt-oss:120b\",\n",
    "    \"--llm-url\", \"http://localhost\",\n",
    "    \"--llm-port\", \"11434\",\n",
    "    \"--encoder-type\", \"hf_local\",\n",
    "    \"--embedding-model\", \"/data/models/nvidia-llama-embed-nemotron-8b\",\n",
    "    \"--secrets-file\", \"/data/coscientist/secrets.toml\",\n",
    "    \"--output-dir\", \"./results/co-storm-notebook\",\n",
    "    \"--enable_log_print\",\n",
    "]\n",
    "\n",
    "parser = ArgumentParser()\n",
    "parser.add_argument(\n",
    "    \"--output-dir\",\n",
    "    type=str,\n",
    "    default=\"./results/co-storm\",\n",
    "    help=\"Directory to store the outputs.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--llm-provider\",\n",
    "    type=str,\n",
    "    choices=[\"ollama\", \"openai\", \"azure\"],\n",
    "    default=\"ollama\",\n",
    "    help=\"LLM provider to use.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--llm-model\",\n",
    "    type=str,\n",
    "    default=\"gpt-oss:120b\",\n",
    "    help=\"Model name for the selected LLM provider (for Ollama, omit the 'ollama/' prefix).\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--llm-url\",\n",
    "    type=str,\n",
    "    default=\"http://localhost\",\n",
    "    help=\"Base URL for the LLM service (used for Ollama).\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--llm-port\",\n",
    "    type=int,\n",
    "    default=11434,\n",
    "    help=\"Port for the LLM service (used for Ollama).\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ollama-model-dir\",\n",
    "    type=str,\n",
    "    default=\"/data/ollama/models\",\n",
    "    help=\"Directory where Ollama should store models.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--llm-temperature\",\n",
    "    type=float,\n",
    "    default=1.0,\n",
    "    help=\"Sampling temperature for the LLM.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--llm-top-p\",\n",
    "    type=float,\n",
    "    default=0.9,\n",
    "    help=\"Top-p for nucleus sampling.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--encoder-type\",\n",
    "    type=str,\n",
    "    choices=[\"hf_local\", \"ollama\", \"openai\", \"azure\"],\n",
    "    default=\"hf_local\",\n",
    "    help=\"Embedding backend to use.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--embedding-model\",\n",
    "    type=str,\n",
    "    default=\"/data/models/nvidia-llama-embed-nemotron-8b\",\n",
    "    help=\"Embedding model name or local path.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--embedding-base-url\",\n",
    "    type=str,\n",
    "    default=\"http://localhost\",\n",
    "    help=\"Base URL for embedding service when encoder-type is ollama.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--embedding-port\",\n",
    "    type=int,\n",
    "    default=11434,\n",
    "    help=\"Port for embedding service when encoder-type is ollama.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--embedding-device\",\n",
    "    type=str,\n",
    "    default=\"auto\",\n",
    "    help=\"Device for local embeddings (auto, cpu, cuda).\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--embedding-cache-dir\",\n",
    "    type=str,\n",
    "    default=\"/data/models\",\n",
    "    help=\"Cache directory / HF_HOME for local embedding models.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--secrets-file\",\n",
    "    type=str,\n",
    "    default=\"/data/coscientist/secrets.toml\",\n",
    "    help=\"Path to secrets.toml for API keys.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--retriever\",\n",
    "    type=str,\n",
    "    choices=[\"bing\", \"you\", \"brave\", \"serper\", \"duckduckgo\", \"tavily\", \"searxng\"],\n",
    "    default=\"duckduckgo\",\n",
    "    help=\"The search engine API to use for retrieving information.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--retrieve_top_k\",\n",
    "    type=int,\n",
    "    default=10,\n",
    "    help=\"Retrieve top k results for each query in retriever.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_search_queries\",\n",
    "    type=int,\n",
    "    default=2,\n",
    "    help=\"Maximum number of search queries to consider for each question.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--total_conv_turn\",\n",
    "    type=int,\n",
    "    default=20,\n",
    "    help=\"Maximum number of turns in conversation.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_search_thread\",\n",
    "    type=int,\n",
    "    default=5,\n",
    "    help=\"Maximum number of parallel threads for retriever.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_search_queries_per_turn\",\n",
    "    type=int,\n",
    "    default=3,\n",
    "    help=\"Maximum number of search queries to consider in each turn.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--warmstart_max_num_experts\",\n",
    "    type=int,\n",
    "    default=3,\n",
    "    help=\"Max number of experts in perspective-guided QA during warm start.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--warmstart_max_turn_per_experts\",\n",
    "    type=int,\n",
    "    default=2,\n",
    "    help=\"Max number of turns per perspective during warm start.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--warmstart_max_thread\",\n",
    "    type=int,\n",
    "    default=3,\n",
    "    help=\"Max number of threads for parallel perspective-guided QA during warm start.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_thread_num\",\n",
    "    type=int,\n",
    "    default=10,\n",
    "    help=(\n",
    "        \"Maximum number of threads to use. \"\n",
    "        \"Consider reducing it if you keep getting 'Exceed rate limit' errors when calling the LM API.\"\n",
    "    ),\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_num_round_table_experts\",\n",
    "    type=int,\n",
    "    default=2,\n",
    "    help=\"Max number of active experts in round table discussion.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--moderator_override_N_consecutive_answering_turn\",\n",
    "    type=int,\n",
    "    default=3,\n",
    "    help=(\n",
    "        \"Number of consecutive expert answering turns before the moderator overrides the conversation.\"\n",
    "    ),\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--node_expansion_trigger_count\",\n",
    "    type=int,\n",
    "    default=10,\n",
    "    help=\"Trigger node expansion for nodes that contain more than N snippets.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--enable_log_print\",\n",
    "    action=\"store_true\",\n",
    "    help=\"If set, enable console log print.\",\n",
    ")\n",
    "\n",
    "args = parser.parse_args(arg_list)\n",
    "main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1052df4-7f45-4f0d-9674-94cd103802bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (coscientist)",
   "language": "python",
   "name": "coscientist"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
